{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(n_workers=1, threads_per_worker=8, processes=False, memory_limit='48GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data'\n",
    "XML_DATA_URL = 'https://afdata.s3.us-gov-west-1.amazonaws.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(url, save_path, chunk_size=128):\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(save_path, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            fd.write(chunk)\n",
    "            \n",
    "def read_files(path, ext):\n",
    "    file_list = []\n",
    "    for root, folders, docs in os.walk(path):\n",
    "        file_list.extend( [os.path.join(root, doc) for doc in docs if ext in doc] )\n",
    "\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download XML list of data sources.\n",
    "xml_data_path = DATA_DIR+'/data_sources.xml'\n",
    "xml_data = download_url(XML_DATA_URL, xml_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download all zip data files from the XML source.\n",
    "with open(xml_data_path, 'r') as xml:\n",
    "    soup = BeautifulSoup(xml, 'xml')\n",
    "    \n",
    "    # XML structure: <Contents><Key>filename</Key><Size>bytes</Size></Contents>\n",
    "    contents_elements = soup.find_all('Contents')\n",
    "    \n",
    "    for contents in contents_elements:\n",
    "        key = contents.find('Key')\n",
    "        filename = str(key.text)\n",
    "        if not re.search(r'\\.zip$', filename):\n",
    "            continue\n",
    "        \n",
    "        save_path = str.format('{0}/{1}', DATA_DIR, filename)\n",
    "        url = str.format('{0}/{1}', XML_DATA_URL, filename)     \n",
    "        expected_size = int(str(contents.find('Size').text))\n",
    "        \n",
    "        # Only download if the file doesn't exist with the expected size in bytes.\n",
    "        if os.path.exists(save_path):\n",
    "            actual_size = os.path.getsize(save_path)\n",
    "            if expected_size == actual_size:\n",
    "                print(str.format('{0} at {1} bytes already exists.', filename, expected_size))\n",
    "                continue\n",
    "        \n",
    "        print(str.format('Downloading {0}...', url))\n",
    "        \n",
    "        download_url(url, save_path)\n",
    "        actual_size = os.path.getsize(save_path)\n",
    "        \n",
    "        if actual_size != expected_size:\n",
    "            print(str.format('WARNING: File size for {0} at {1} bytes does not match the expected size of {2} bytes.',\n",
    "                            filename, actual_size, expected_size))\n",
    "        else:  \n",
    "            print(str.format('Successfully downloaded {0} to {1}. Filesize: {2} bytes.'\n",
    "                             , url, save_path, actual_size))\n",
    "        \n",
    "    xml.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip all data files.\n",
    "for path in read_files(path=DATA_DIR, ext='.zip'):\n",
    "    extract_path = '/'.join(str(path).rsplit('/')[:-1]) + '/unzipped/'\n",
    "    with ZipFile(path, 'r') as zipfile:\n",
    "        zipfile.extractall(extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/Scenario_Data/TLE/unzipped/tle2004_1of8.clean.txt\n",
      "./data/Scenario_Data/TLE/unzipped/tle2004_2of8.clean.txt\n",
      "./data/Scenario_Data/TLE/unzipped/tle2004_3of8.clean.txt\n",
      "./data/Scenario_Data/TLE/unzipped/tle2004_4of8.clean.txt\n",
      "./data/Scenario_Data/TLE/unzipped/tle2004_5of8.clean.txt\n",
      "./data/Scenario_Data/TLE/unzipped/tle2004_6of8.clean.txt\n"
     ]
    }
   ],
   "source": [
    "# Clean TLE data, Save as pipe delimitted datasets.\n",
    "tle_files = read_files('./data/Scenario_Data/TLE/unzipped', '.txt')\n",
    "for path in tle_files:\n",
    "    filename = re.sub(r'\\.txt$', '.clean.txt', path)\n",
    "    with open(path, 'r') as file:\n",
    "        with open(filename, 'w', encoding='utf-8') as newfile:\n",
    "            while True:\n",
    "                line = file.readline()\n",
    "                if not line:\n",
    "                    file.close()\n",
    "                    newfile.close()\n",
    "                    print(filename)\n",
    "                    break\n",
    "                \n",
    "                newline = ''\n",
    "                line2 = False\n",
    "                if re.search(r'^2', line):\n",
    "                    newline = '\\r\\n'\n",
    "                    line2 = True\n",
    "   \n",
    "                line = re.sub(r'^(1|2)\\s+', '', line)\n",
    "                line = re.sub(r'(^\\s+|\\s+$)', '', line)\n",
    "                line = re.sub(r'\\\\', '|', line)\n",
    "                line = re.sub(r'\\s+', '|', line)   \n",
    "\n",
    "                # Sometimes Mean Motion and Revolution number are crammed together. Separate.\n",
    "                rx = r'(?<=\\|)\\d{1,2}\\.\\d{13}'\n",
    "                if line2 and re.search(r'(?<=\\|)\\d{1,2}\\.\\d{13}', line):\n",
    "                    last_num = re.findall(rx, line)[0]\n",
    "                    mean_motion = last_num[:-5]\n",
    "                    rev_num = last_num[-5:]\n",
    "                    # print(mean_motion, rev_num)\n",
    "                    line = re.sub(rx, mean_motion+'|'+rev_num, line)\n",
    "                    \n",
    "                line = line + newline\n",
    "                newfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tle_dtypes = {\n",
    "    'SatNum': str\n",
    "    , 'IntnlDesignator': str\n",
    "    , 'EpochYear': np.float\n",
    "    , 'JulianDayFrac':  np.float\n",
    "    , 'BallisticCoef': np.float\n",
    "    , 'SecDerivMeanMotion': str\n",
    "    , 'DragTerm': str\n",
    "    , 'EphemerisType': np.int\n",
    "    , 'ElemNumCheckSum': np.int\n",
    "    , 'SatNum2': str\n",
    "    , 'Inclination': np.float\n",
    "    , 'RightAscension': np.float\n",
    "    , 'Eccentricity': str\n",
    "    , 'ArgPerigree': np.float\n",
    "    , 'MeanAnomaly': np.float\n",
    "    , 'MeanMotion': np.float\n",
    "    , 'RevNumEpochCheckSum': np.int\n",
    "}\n",
    "\n",
    "tle_columns = [col for col in tle_dtypes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tle_data = dd.read_csv('./data/Scenario_Data/TLE/unzipped/*.clean.txt'\n",
    "                       , sep='|', header=None, names=tle_columns, dtype=tle_dtypes, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
