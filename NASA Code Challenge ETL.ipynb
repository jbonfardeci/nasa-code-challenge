{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "from datetime import datetime, timedelta\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data'\n",
    "XML_DATA_URL = 'https://afdata.s3.us-gov-west-1.amazonaws.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(url, save_path, chunk_size=128):\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(save_path, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            fd.write(chunk)\n",
    "            \n",
    "def read_files(path, ext):\n",
    "    file_list = []\n",
    "    for root, folders, docs in os.walk(path):\n",
    "        file_list.extend( [os.path.join(root, doc) for doc in docs if ext in doc] )\n",
    "\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download XML list of data sources.\n",
    "xml_data_path = DATA_DIR+'/data_sources.xml'\n",
    "xml_data = download_url(XML_DATA_URL, xml_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download all zip data files from the XML source.\n",
    "with open(xml_data_path, 'r') as xml:\n",
    "    soup = BeautifulSoup(xml, 'xml')\n",
    "    \n",
    "    # XML structure: <Contents><Key>filename</Key><Size>bytes</Size></Contents>\n",
    "    contents_elements = soup.find_all('Contents')\n",
    "    \n",
    "    for contents in contents_elements:\n",
    "        key = contents.find('Key')\n",
    "        filename = str(key.text)\n",
    "        if not re.search(r'\\.zip$', filename):\n",
    "            continue\n",
    "        \n",
    "        save_path = str.format('{0}/{1}', DATA_DIR, filename)\n",
    "        url = str.format('{0}/{1}', XML_DATA_URL, filename)     \n",
    "        expected_size = int(str(contents.find('Size').text))\n",
    "        \n",
    "        # Only download if the file doesn't exist with the expected size in bytes.\n",
    "        if os.path.exists(save_path):\n",
    "            actual_size = os.path.getsize(save_path)\n",
    "            if expected_size == actual_size:\n",
    "                print(str.format('{0} at {1} bytes already exists.', filename, expected_size))\n",
    "                continue\n",
    "        \n",
    "        print(str.format('Downloading {0}...', url))\n",
    "        \n",
    "        download_url(url, save_path)\n",
    "        actual_size = os.path.getsize(save_path)\n",
    "        \n",
    "        if actual_size != expected_size:\n",
    "            print(str.format('WARNING: File size for {0} at {1} bytes does not match the expected size of {2} bytes.',\n",
    "                            filename, actual_size, expected_size))\n",
    "        else:  \n",
    "            print(str.format('Successfully downloaded {0} to {1}. Filesize: {2} bytes.'\n",
    "                             , url, save_path, actual_size))\n",
    "        \n",
    "    xml.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip all data files.\n",
    "for path in read_files(path=DATA_DIR, ext='.zip'):\n",
    "    extract_path = '/'.join(str(path).rsplit('/')[:-1]) + '/unzipped/'\n",
    "    with ZipFile(path, 'r') as zipfile:\n",
    "        zipfile.extractall(extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned ./data/Scenario_Data/TLE/unzipped/tle2004_1of8.clean.txt\n",
      "Cleaned ./data/Scenario_Data/TLE/unzipped/tle2004_2of8.clean.txt\n",
      "Cleaned ./data/Scenario_Data/TLE/unzipped/tle2004_3of8.clean.txt\n",
      "Cleaned ./data/Scenario_Data/TLE/unzipped/tle2004_4of8.clean.txt\n",
      "Cleaned ./data/Scenario_Data/TLE/unzipped/tle2004_5of8.clean.txt\n",
      "Cleaned ./data/Scenario_Data/TLE/unzipped/tle2004_6of8.clean.txt\n",
      "Cleaned ./data/Scenario_Data/TLE/unzipped/tle2004_7of8.clean.txt\n",
      "Cleaned ./data/Scenario_Data/TLE/unzipped/tle2004_8of8.clean.txt\n",
      "Cleaned ./data/Scenario_Data/TLE/unzipped/tle2005.clean.txt\n",
      "Cleaned ./data/Scenario_Data/TLE/unzipped/tle2006.clean.txt\n",
      "Cleaned ./data/Scenario_Data/TLE/unzipped/tle2007.clean.txt\n",
      "Cleaned ./data/Scenario_Data/TLE/unzipped/tle2008.clean.txt\n",
      "Cleaned ./data/Scenario_Data/TLE/unzipped/tle2009.clean.txt\n",
      "Cleaned ./data/Scenario_Data/TLE/unzipped/tle2010.clean.txt\n",
      "Cleaned ./data/Scenario_Data/TLE/unzipped/tle2011.clean.txt\n",
      "Cleaned ./data/Scenario_Data/TLE/unzipped/tle2012.clean.txt\n",
      "Cleaned ./data/Scenario_Data/TLE/unzipped/tle2013.clean.txt\n",
      "Cleaned ./data/Scenario_Data/TLE/unzipped/tle2014.clean.txt\n",
      "Cleaned ./data/Scenario_Data/TLE/unzipped/tle2015.clean.txt\n",
      "Cleaned ./data/Scenario_Data/TLE/unzipped/tle2016.clean.txt\n",
      "Cleaned ./data/Scenario_Data/TLE/unzipped/tle2017.clean.txt\n",
      "Cleaned ./data/Scenario_Data/TLE/unzipped/tle2018.clean.txt\n"
     ]
    }
   ],
   "source": [
    "def julian_to_iso(julian):\n",
    "    yr = int(julian[:2])\n",
    "    yr = (2000+yr) if yr < 21 else (1900+yr)\n",
    "    day = math.floor(float(julian[2:]))\n",
    "    fraction = float('.'+julian.split('.')[1])\n",
    "    dec_hours = fraction*24\n",
    "    startdate = datetime(year=yr, month=1, day=1)\n",
    "    startdate += timedelta(days=day)\n",
    "    startdate += timedelta(hours=dec_hours)\n",
    "    return startdate.isoformat()\n",
    "    \n",
    "def clean_intnl_designator(line):\n",
    "    \"\"\"\n",
    "    Extract LaunchYear, NthLaunch, CharLaunchObject from International Designator.\n",
    "        e.g. International Designator = '84123A' \n",
    "        where '84' is launch yr, \n",
    "        '123' is nth launch, \n",
    "        and 'A' is nth object resulting from this launch.\n",
    "    \"\"\"\n",
    "    intnl_desig = re.findall(r'(?<=\\|)\\d{5}[A-Z]+}(?=\\|)', line)\n",
    "    if len(intnl_desig) > 0:\n",
    "        val = intnl_desig[0]\n",
    "        launch_year = int(val[:2])\n",
    "        launch_year = 2000 + launch_year if (launch_year < 21) else (1900 + launch_year)\n",
    "        nth_launch = int(val[2:5])\n",
    "        char_launch_obj = re.sub(r'[^A-Z]', '', val)\n",
    "        return line.replace(val, str.format(\"{0}|{1}|{2}\", launch_year, nth_launch, char_launch_obj))\n",
    "    \n",
    "    return line\n",
    " \n",
    "def clean_mean_motion(line):\n",
    "    \"\"\"\n",
    "    Sometimes Mean Motion and Revolution number are crammed together. Separate them.\n",
    "    \"\"\"\n",
    "    last_num = re.findall(r'(?<=\\|)\\d{1,2}\\.\\d{13}', line)\n",
    "    if len(last_num) > 0:\n",
    "        n = last_num[0]\n",
    "        mean_motion = n[:-5]\n",
    "        rev_num = n[-5:]\n",
    "        return line.replace(n, mean_motion+'|'+rev_num)\n",
    "    \n",
    "    return line\n",
    "        \n",
    "def clean_tle_line(line):\n",
    "    newline = ''\n",
    "    line2 = False\n",
    "    if re.search(r'^2', line):\n",
    "        newline = '\\r\\n'\n",
    "        line2 = True\n",
    "\n",
    "    line = re.sub(r'^(1|2)\\s+', '', line)\n",
    "    line = re.sub(r'(^\\s+|\\s+$)', '', line)\n",
    "    line = re.sub(r'\\\\', '|', line)\n",
    "    line = re.sub(r'\\s+', '|', line) \n",
    "\n",
    "    # Separate Mean Motion and Revolution Number.\n",
    "    if line2:\n",
    "        line = clean_mean_motion(line)\n",
    "\n",
    "    # Get International Designator parts\n",
    "    line = clean_intnl_designator(line)\n",
    "    \n",
    "    # Convert Julian date to ISO\n",
    "    if not line2:\n",
    "        rx_julian = re.findall(r'(?<=\\|)\\d{3,5}\\.\\d{8}(?=\\|)', line)\n",
    "        if len(rx_julian) > 0:\n",
    "            julian_date = rx_julian[0]\n",
    "            iso = julian_to_iso(julian_date)\n",
    "            line = line.replace(julian_date, iso)\n",
    "    \n",
    "    return line + newline\n",
    "\n",
    "# Clean TLE data. Save as pipe delimitted datasets.\n",
    "tle_files = read_files('./data/Scenario_Data/TLE/unzipped', '.txt')\n",
    "# tle_files = ['./data/Scenario_Data/TLE/test.txt']\n",
    "\n",
    "for path in tle_files:\n",
    "    filename = re.sub(r'\\.txt$', '.clean.txt', path)\n",
    "    with open(path, 'r') as file:\n",
    "        with open(filename, 'w', encoding='utf-8') as newfile:\n",
    "            while True:\n",
    "                line = file.readline()\n",
    "                if not line:\n",
    "                    file.close()\n",
    "                    newfile.close()\n",
    "                    print('Cleaned', filename)\n",
    "                    break\n",
    "\n",
    "                cleaned_line = clean_tle_line(line)\n",
    "                newfile.write(cleaned_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '94029ACD'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot cast array data from dtype('O') to dtype('int64') according to the rule 'safe'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-b0841b1a60aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mtle_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtle_dtypes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m tle_data = dd.read_csv('./data/Scenario_Data/TLE/unzipped/*.clean.txt'\n\u001b[0m\u001b[1;32m     25\u001b[0m                        \u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtle_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                        \u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtle_dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/datasci/lib/python3.8/site-packages/dask/dataframe/io/csv.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(urlpath, blocksize, lineterminator, compression, sample, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     ):\n\u001b[0;32m--> 700\u001b[0;31m         return read_pandas(\n\u001b[0m\u001b[1;32m    701\u001b[0m             \u001b[0mreader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0murlpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/datasci/lib/python3.8/site-packages/dask/dataframe/io/csv.py\u001b[0m in \u001b[0;36mread_pandas\u001b[0;34m(reader, urlpath, blocksize, lineterminator, compression, sample, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;31m# Use sample to infer dtypes and check for presence of include_path_column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minclude_path_column\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minclude_path_column\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/.venvs/datasci/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/datasci/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/datasci/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/datasci/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '94029ACD'"
     ]
    }
   ],
   "source": [
    "tle_dtypes = {\n",
    "    'SatID': 'object'\n",
    "    , 'LaunchYear': np.int\n",
    "    , 'NthLaunch': np.int\n",
    "    , 'CharLaunchObject': 'object'\n",
    "    , 'EpochYear': 'object'\n",
    "    , 'BallisticCoef': 'object'\n",
    "    , 'SecDerivMeanMotion': 'object'\n",
    "    , 'DragTerm': 'object'\n",
    "    , 'EphemerisType': np.int\n",
    "    , 'ElemNumCheckSum': np.int\n",
    "    , 'SatNumID': 'object'\n",
    "    , 'Inclination': np.float\n",
    "    , 'RightAscension': np.float\n",
    "    , 'Eccentricity': np.float\n",
    "    , 'ArgPerigree': np.float\n",
    "    , 'MeanAnomaly': np.float\n",
    "    , 'MeanMotion': np.float\n",
    "    , 'RevNumEpochCheckSum': np.float\n",
    "}\n",
    "\n",
    "tle_columns = [col for col in tle_dtypes]\n",
    "\n",
    "tle_data = dd.read_csv('./data/Scenario_Data/TLE/unzipped/*.clean.txt'\n",
    "                       , names=tle_columns\n",
    "                       , dtype=tle_dtypes\n",
    "                       , sep='|'\n",
    "                       , encoding='utf-8'\n",
    "                      )\n",
    "\n",
    "tle_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2018 TLE data\n",
    "tle_data = dd.read_csv('./data/Scenario_Data/TLE/unzipped/*.clean.txt'\n",
    "                       , names=tle_columns\n",
    "                       , dtype=tle_dtypes\n",
    "                       , sep='|'\n",
    "                       , encoding='utf-8'\n",
    "                      )\n",
    "\n",
    "tle_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get AIS data\n",
    "\n",
    "# Relocate CSV files to main AIS directory.\n",
    "ais_files = read_files('./data/Scenario_Data/AIS/unzipped/AIS_ASCII_by_UTM_Month', '.csv')\n",
    "for path in ais_files:\n",
    "    filename = os.path.basename(path)\n",
    "    print(filename)\n",
    "    shutil.move(path, './data/Scenario_Data/AIS/'+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MMSI</th>\n",
       "      <th>BaseDateTime</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LON</th>\n",
       "      <th>SOG</th>\n",
       "      <th>COG</th>\n",
       "      <th>Heading</th>\n",
       "      <th>VesselName</th>\n",
       "      <th>IMO</th>\n",
       "      <th>CallSign</th>\n",
       "      <th>VesselType</th>\n",
       "      <th>Status</th>\n",
       "      <th>Length</th>\n",
       "      <th>Width</th>\n",
       "      <th>Draft</th>\n",
       "      <th>Cargo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>235091871.0</td>\n",
       "      <td>2015-01-01T00:08:26</td>\n",
       "      <td>52.78763</td>\n",
       "      <td>-175.62761</td>\n",
       "      <td>10.3</td>\n",
       "      <td>74.5</td>\n",
       "      <td>86.0</td>\n",
       "      <td>EVA BULKER</td>\n",
       "      <td>IMO9544164</td>\n",
       "      <td>2FJU4</td>\n",
       "      <td>70.0</td>\n",
       "      <td>under way using engine</td>\n",
       "      <td>185.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>247119100.0</td>\n",
       "      <td>2015-01-01T05:36:17</td>\n",
       "      <td>52.87994</td>\n",
       "      <td>-176.21738</td>\n",
       "      <td>10.7</td>\n",
       "      <td>-148.8</td>\n",
       "      <td>263.0</td>\n",
       "      <td>POLE</td>\n",
       "      <td>IMO9128245</td>\n",
       "      <td>IBTE</td>\n",
       "      <td>70.0</td>\n",
       "      <td>under way using engine</td>\n",
       "      <td>224.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-12.8</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>247119100.0</td>\n",
       "      <td>2015-01-01T06:28:57</td>\n",
       "      <td>52.83234</td>\n",
       "      <td>-176.46662</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-160.8</td>\n",
       "      <td>254.0</td>\n",
       "      <td>POLE</td>\n",
       "      <td>IMO9128245</td>\n",
       "      <td>IBTE</td>\n",
       "      <td>70.0</td>\n",
       "      <td>under way using engine</td>\n",
       "      <td>224.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-12.8</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>247119100.0</td>\n",
       "      <td>2015-01-01T06:32:27</td>\n",
       "      <td>52.82851</td>\n",
       "      <td>-176.48291</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-160.6</td>\n",
       "      <td>254.0</td>\n",
       "      <td>POLE</td>\n",
       "      <td>IMO9128245</td>\n",
       "      <td>IBTE</td>\n",
       "      <td>70.0</td>\n",
       "      <td>under way using engine</td>\n",
       "      <td>224.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-12.8</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>247119100.0</td>\n",
       "      <td>2015-01-01T06:36:07</td>\n",
       "      <td>52.82446</td>\n",
       "      <td>-176.50022</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-160.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>POLE</td>\n",
       "      <td>IMO9128245</td>\n",
       "      <td>IBTE</td>\n",
       "      <td>70.0</td>\n",
       "      <td>under way using engine</td>\n",
       "      <td>224.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-12.8</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          MMSI         BaseDateTime       LAT        LON   SOG    COG  \\\n",
       "0  235091871.0  2015-01-01T00:08:26  52.78763 -175.62761  10.3   74.5   \n",
       "1  247119100.0  2015-01-01T05:36:17  52.87994 -176.21738  10.7 -148.8   \n",
       "2  247119100.0  2015-01-01T06:28:57  52.83234 -176.46662  11.0 -160.8   \n",
       "3  247119100.0  2015-01-01T06:32:27  52.82851 -176.48291  11.0 -160.6   \n",
       "4  247119100.0  2015-01-01T06:36:07  52.82446 -176.50022  11.0 -160.0   \n",
       "\n",
       "   Heading  VesselName         IMO CallSign  VesselType  \\\n",
       "0     86.0  EVA BULKER  IMO9544164    2FJU4        70.0   \n",
       "1    263.0        POLE  IMO9128245     IBTE        70.0   \n",
       "2    254.0        POLE  IMO9128245     IBTE        70.0   \n",
       "3    254.0        POLE  IMO9128245     IBTE        70.0   \n",
       "4    254.0        POLE  IMO9128245     IBTE        70.0   \n",
       "\n",
       "                   Status  Length  Width  Draft  Cargo  \n",
       "0  under way using engine   185.0   31.0    6.6   70.0  \n",
       "1  under way using engine   224.0   32.0  -12.8   70.0  \n",
       "2  under way using engine   224.0   32.0  -12.8   70.0  \n",
       "3  under way using engine   224.0   32.0  -12.8   70.0  \n",
       "4  under way using engine   224.0   32.0  -12.8   70.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine AIS CSV files.\n",
    "ais_data = dd.read_csv('./data/Scenario_Data/AIS/*.csv', assume_missing=True)\n",
    "ais_data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
