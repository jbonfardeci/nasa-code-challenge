{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster, Worker\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0murlpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mblocksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0menforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0massume_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minclude_path_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Read CSV files into a Dask.DataFrame\n",
       "\n",
       "This parallelizes the :func:`pandas.read_csv` function in the following ways:\n",
       "\n",
       "- It supports loading many files at once using globstrings:\n",
       "\n",
       "    >>> df = dd.read_csv('myfiles.*.csv')  # doctest: +SKIP\n",
       "\n",
       "- In some cases it can break up large files:\n",
       "\n",
       "    >>> df = dd.read_csv('largefile.csv', blocksize=25e6)  # 25MB chunks  # doctest: +SKIP\n",
       "\n",
       "- It can read CSV files from external resources (e.g. S3, HDFS) by\n",
       "  providing a URL:\n",
       "\n",
       "    >>> df = dd.read_csv('s3://bucket/myfiles.*.csv')  # doctest: +SKIP\n",
       "    >>> df = dd.read_csv('hdfs:///myfiles.*.csv')  # doctest: +SKIP\n",
       "    >>> df = dd.read_csv('hdfs://namenode.example.com/myfiles.*.csv')  # doctest: +SKIP\n",
       "\n",
       "Internally ``dd.read_csv`` uses :func:`pandas.read_csv` and supports many of the\n",
       "same keyword arguments with the same performance guarantees. See the docstring\n",
       "for :func:`pandas.read_csv` for more information on available keyword arguments.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "urlpath : string or list\n",
       "    Absolute or relative filepath(s). Prefix with a protocol like ``s3://``\n",
       "    to read from alternative filesystems. To read from multiple files you\n",
       "    can pass a globstring or a list of paths, with the caveat that they\n",
       "    must all have the same protocol.\n",
       "blocksize : str, int or None, optional\n",
       "    Number of bytes by which to cut up larger files. Default value is computed\n",
       "    based on available physical memory and the number of cores, up to a maximum\n",
       "    of 64MB. Can be a number like ``64000000` or a string like ``\"64MB\"``. If\n",
       "    ``None``, a single block is used for each file.\n",
       "sample : int, optional\n",
       "    Number of bytes to use when determining dtypes\n",
       "assume_missing : bool, optional\n",
       "    If True, all integer columns that aren't specified in ``dtype`` are assumed\n",
       "    to contain missing values, and are converted to floats. Default is False.\n",
       "storage_options : dict, optional\n",
       "    Extra options that make sense for a particular storage connection, e.g.\n",
       "    host, port, username, password, etc.\n",
       "include_path_column : bool or str, optional\n",
       "    Whether or not to include the path to each particular file. If True a new\n",
       "    column is added to the dataframe called ``path``. If str, sets new column\n",
       "    name. Default is False.\n",
       "**kwargs\n",
       "    Extra keyword arguments to forward to :func:`pandas.read_csv`.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "Dask dataframe tries to infer the ``dtype`` of each column by reading a sample\n",
       "from the start of the file (or of the first file if it's a glob). Usually this\n",
       "works fine, but if the ``dtype`` is different later in the file (or in other\n",
       "files) this can cause issues. For example, if all the rows in the sample had\n",
       "integer dtypes, but later on there was a ``NaN``, then this would error at\n",
       "compute time. To fix this, you have a few options:\n",
       "\n",
       "- Provide explicit dtypes for the offending columns using the ``dtype``\n",
       "  keyword. This is the recommended solution.\n",
       "\n",
       "- Use the ``assume_missing`` keyword to assume that all columns inferred as\n",
       "  integers contain missing values, and convert them to floats.\n",
       "\n",
       "- Increase the size of the sample using the ``sample`` keyword.\n",
       "\n",
       "It should also be noted that this function may fail if a CSV file\n",
       "includes quoted strings that contain the line terminator. To get around this\n",
       "you can specify ``blocksize=None`` to not split files into multiple partitions,\n",
       "at the cost of reduced parallelism.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.venvs/datasci/lib/python3.8/site-packages/dask/dataframe/io/csv.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dd.read_csv?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
